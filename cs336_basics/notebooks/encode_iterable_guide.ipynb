{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Руководство по Iterable, Iterator и encode_iterable\n",
        "\n",
        "Этот ноутбук объясняет концепции **Iterable** и **Iterator** в Python и показывает, где может пригодиться метод `encode_iterable` из класса `Tokenizer`.\n",
        "\n",
        "## Содержание:\n",
        "1. [Основы Iterable и Iterator](#basics)\n",
        "2. [Генераторы и ленивые вычисления](#generators)\n",
        "3. [Практические примеры encode_iterable](#examples)\n",
        "4. [Эффективность памяти](#memory)\n",
        "5. [Пайплайны обработки данных](#pipelines)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Основы Iterable и Iterator {#basics}\n",
        "\n",
        "### Что такое Iterable?\n",
        "**Iterable** (итерируемый) - это объект, по которому можно итерироваться (проходить элемент за элементом). Все iterable объекты имеют метод `__iter__()`, который возвращает iterator.\n",
        "\n",
        "### Что такое Iterator?\n",
        "**Iterator** (итератор) - это объект, который производит значения по одному за раз. У него есть методы:\n",
        "- `__iter__()` - возвращает сам себя\n",
        "- `__next__()` - возвращает следующий элемент или поднимает `StopIteration`\n",
        "\n",
        "### Основные встроенные итерируемые объекты:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Список: [1, 2, 3, 4, 5]\n",
            "Строка: ['П', 'р', 'и', 'в', 'е', 'т']\n",
            "Ключи словаря: ['a', 'b', 'c']\n",
            "Значения словаря: [1, 2, 3]\n",
            "\n",
            "Список итерируемый: True\n",
            "Строка итерируемая: True\n",
            "Словарь итерируемый: True\n"
          ]
        }
      ],
      "source": [
        "# Примеры встроенных итерируемых объектов\n",
        "my_list = [1, 2, 3, 4, 5]\n",
        "my_string = \"Привет\"\n",
        "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "\n",
        "print(\"Список:\", list(my_list))\n",
        "print(\"Строка:\", list(my_string))\n",
        "print(\"Ключи словаря:\", list(my_dict.keys()))\n",
        "print(\"Значения словаря:\", list(my_dict.values()))\n",
        "\n",
        "# Проверим, являются ли они итерируемыми\n",
        "print(f\"\\nСписок итерируемый: {hasattr(my_list, '__iter__')}\")\n",
        "print(f\"Строка итерируемая: {hasattr(my_string, '__iter__')}\")\n",
        "print(f\"Словарь итерируемый: {hasattr(my_dict, '__iter__')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тип итератора: <class 'list_iterator'>\n",
            "У итератора есть __iter__: True\n",
            "У итератора есть __next__: True\n",
            "\n",
            "Ручная итерация:\n",
            "Первый элемент: 10\n",
            "Второй элемент: 20\n",
            "Третий элемент: 30\n",
            "Четвёртый элемент: 40\n",
            "Достигнут конец итератора - поднято исключение StopIteration\n"
          ]
        }
      ],
      "source": [
        "# Получение итератора и работа с ним\n",
        "my_list = [10, 20, 30, 40]\n",
        "\n",
        "# Получаем итератор из списка\n",
        "list_iterator = iter(my_list)\n",
        "print(f\"Тип итератора: {type(list_iterator)}\")\n",
        "\n",
        "# Проверяем наличие методов итератора\n",
        "print(f\"У итератора есть __iter__: {hasattr(list_iterator, '__iter__')}\")\n",
        "print(f\"У итератора есть __next__: {hasattr(list_iterator, '__next__')}\")\n",
        "\n",
        "# Ручная итерация с помощью next()\n",
        "print(\"\\nРучная итерация:\")\n",
        "print(f\"Первый элемент: {next(list_iterator)}\")\n",
        "print(f\"Второй элемент: {next(list_iterator)}\")\n",
        "print(f\"Третий элемент: {next(list_iterator)}\")\n",
        "\n",
        "# Можем продолжить итерацию\n",
        "print(f\"Четвёртый элемент: {next(list_iterator)}\")\n",
        "\n",
        "# Попытка получить следующий элемент вызовет StopIteration\n",
        "try:\n",
        "    print(f\"Пятый элемент: {next(list_iterator)}\")\n",
        "except StopIteration:\n",
        "    print(\"Достигнут конец итератора - поднято исключение StopIteration\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Генераторы и ленивые вычисления {#generators}\n",
        "\n",
        "**Генераторы** - это удобный способ создания итераторов в Python. Они используют ключевое слово `yield` и автоматически реализуют протокол итератора.\n",
        "\n",
        "### Главные преимущества генераторов:\n",
        "- **Ленивые вычисления** - значения вычисляются только по запросу\n",
        "- **Эффективность памяти** - не нужно хранить все значения одновременно\n",
        "- **Простота создания** - не нужно писать классы с `__iter__` и `__next__`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример простого генератора\n",
        "def simple_generator():\n",
        "    \"\"\"Простой генератор с yield\"\"\"\n",
        "    print(\"Генерируем 1\")\n",
        "    yield 1\n",
        "    print(\"Генерируем 2\") \n",
        "    yield 2\n",
        "    print(\"Генерируем 3\")\n",
        "    yield 3\n",
        "    print(\"Генератор завершён\")\n",
        "\n",
        "# Создаём генератор (пока ничего не выполняется)\n",
        "gen = simple_generator()\n",
        "print(f\"Тип генератора: {type(gen)}\")\n",
        "print(\"Генератор создан, но код ещё не выполнился\\n\")\n",
        "\n",
        "# Теперь запускаем итерацию\n",
        "print(\"Начинаем итерацию:\")\n",
        "for value in gen:\n",
        "    print(f\"Получили значение: {value}\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Демонстрация ленивых вычислений vs жадных\n",
        "def expensive_operation(x):\n",
        "    \"\"\"Имитируем дорогую операцию\"\"\"\n",
        "    print(f\"Выполняем дорогую операцию для {x}\")\n",
        "    return x ** 2\n",
        "\n",
        "print(\"=== ЖАДНЫЙ ПОДХОД (List Comprehension) ===\")\n",
        "print(\"Создаём список - все вычисления сразу:\")\n",
        "eager_results = [expensive_operation(x) for x in range(5)]\n",
        "print(f\"Результат: {eager_results}\")\n",
        "print()\n",
        "\n",
        "print(\"=== ЛЕНИВЫЙ ПОДХОД (Generator Expression) ===\")\n",
        "print(\"Создаём генератор - вычисления откладываются:\")\n",
        "lazy_results = (expensive_operation(x) for x in range(5))\n",
        "print(f\"Генератор создан: {lazy_results}\")\n",
        "print(\"Никаких вычислений пока не было!\")\n",
        "print()\n",
        "\n",
        "print(\"Теперь берём только первые 2 элемента:\")\n",
        "for i, result in enumerate(lazy_results):\n",
        "    if i >= 2:\n",
        "        break\n",
        "    print(f\"Результат {i}: {result}\")\n",
        "\n",
        "print(\"\\nОстальные 3 элемента не были вычислены - экономия ресурсов!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Практические примеры encode_iterable {#examples}\n",
        "\n",
        "Теперь рассмотрим, где может пригодиться метод `encode_iterable` из вашего класса `Tokenizer`. Этот метод принимает итерируемый объект строк и возвращает итератор токенов.\n",
        "\n",
        "### Сигнатура метода:\n",
        "```python\n",
        "def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
        "    for text in iterable:\n",
        "        for token_id in self.encode(text):\n",
        "            yield token_id\n",
        "```\n",
        "\n",
        "### Основные сценарии использования:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример 1: Обработка большого файла построчно\n",
        "import os\n",
        "\n",
        "# Создаём тестовый файл с множеством строк\n",
        "print(\"Создаём большой текстовый файл...\")\n",
        "with open('large_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for i in range(1000):\n",
        "        f.write(f\"Это строка номер {i}. Здесь много текста для токенизации.\\n\")\n",
        "\n",
        "print(\"Файл создан!\")\n",
        "\n",
        "# Функция для чтения файла построчно (генератор)\n",
        "def read_file_lines(filename):\n",
        "    \"\"\"Генератор, который читает файл построчно\"\"\"\n",
        "    print(f\"Открываем файл {filename}\")\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line_no, line in enumerate(f, 1):\n",
        "            yield line.strip()\n",
        "\n",
        "# Демонстрация ленивого чтения\n",
        "lines_generator = read_file_lines('large_text.txt')\n",
        "print(f\"Тип генератора: {type(lines_generator)}\")\n",
        "\n",
        "# Читаем только первые 3 строки\n",
        "print(\"\\nПервые 3 строки из файла:\")\n",
        "for i, line in enumerate(lines_generator):\n",
        "    if i >= 3:\n",
        "        break\n",
        "    print(f\"Строка {i+1}: {line}\")\n",
        "\n",
        "print(\"\\nОстальные 997 строк не были загружены в память!\")\n",
        "\n",
        "# Здесь мы бы использовали encode_iterable:\n",
        "# tokenizer = Tokenizer.from_files('vocab.txt', 'merges.txt')\n",
        "# token_stream = tokenizer.encode_iterable(lines_generator)\n",
        "\n",
        "# Очистка\n",
        "os.remove('large_text.txt')\n",
        "print(\"Файл удалён\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример 2: Потоковая обработка данных\n",
        "import time\n",
        "\n",
        "def data_stream_simulator():\n",
        "    \"\"\"Симулируем поток данных (например, сообщения из чата или сети)\"\"\"\n",
        "    messages = [\n",
        "        \"Привет, как дела?\",\n",
        "        \"Сегодня хорошая погода\",\n",
        "        \"Завтра пойдём в кино\",\n",
        "        \"Мне нравится программирование на Python\",\n",
        "        \"Машинное обучение очень интересно\"\n",
        "    ]\n",
        "    \n",
        "    for i, msg in enumerate(messages):\n",
        "        print(f\"📥 Получено сообщение {i+1}: {msg}\")\n",
        "        # Имитируем задержку получения данных\n",
        "        time.sleep(0.5)\n",
        "        yield msg\n",
        "\n",
        "print(\"=== Потоковая обработка сообщений ===\")\n",
        "print(\"Сообщения поступают по одному...\")\n",
        "\n",
        "# Создаём поток сообщений\n",
        "message_stream = data_stream_simulator()\n",
        "\n",
        "# В реальном приложении здесь был бы вызов:\n",
        "# tokenizer = Tokenizer.from_files('vocab.txt', 'merges.txt') \n",
        "# token_stream = tokenizer.encode_iterable(message_stream)\n",
        "\n",
        "# Имитируем обработку потока\n",
        "print(\"\\n🔄 Обрабатываем сообщения по мере поступления:\")\n",
        "for i, message in enumerate(message_stream):\n",
        "    print(f\"✅ Обработано: '{message}' (длина: {len(message)} символов)\")\n",
        "    if i >= 2:  # Обрабатываем только первые 3 сообщения\n",
        "        print(\"⏹️  Остановка обработки...\")\n",
        "        break\n",
        "\n",
        "print(\"\\nОстальные сообщения можно обработать позже или не обрабатывать вовсе!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Эффективность памяти {#memory}\n",
        "\n",
        "Одно из главных преимуществ `encode_iterable` - **эффективность памяти**. Вместо загрузки всех данных сразу, обработка происходит по элементам.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Демонстрация эффективности памяти\n",
        "import sys\n",
        "\n",
        "print(\"=== Сравнение использования памяти ===\")\n",
        "\n",
        "# 1. Неэффективный способ - загружаем всё в память\n",
        "def create_large_list(size):\n",
        "    \"\"\"Создаём большой список строк\"\"\"\n",
        "    return [f\"Строка номер {i} с длинным текстом для демонстрации\" for i in range(size)]\n",
        "\n",
        "# 2. Эффективный способ - генератор\n",
        "def create_large_generator(size):\n",
        "    \"\"\"Создаём генератор строк\"\"\"\n",
        "    for i in range(size):\n",
        "        yield f\"Строка номер {i} с длинным текстом для демонстрации\"\n",
        "\n",
        "# Тестируем с 100,000 элементов\n",
        "size = 100_000\n",
        "\n",
        "print(f\"Создаём коллекцию из {size:,} строк...\")\n",
        "\n",
        "# Измеряем размер списка\n",
        "large_list = create_large_list(size)\n",
        "list_size = sys.getsizeof(large_list)\n",
        "print(f\"📦 Размер списка: {list_size:,} байт ({list_size / 1024 / 1024:.1f} МБ)\")\n",
        "\n",
        "# Измеряем размер генератора\n",
        "large_generator = create_large_generator(size)\n",
        "generator_size = sys.getsizeof(large_generator)\n",
        "print(f\"⚡ Размер генератора: {generator_size:,} байт ({generator_size / 1024:.1f} КБ)\")\n",
        "\n",
        "# Вычисляем экономию\n",
        "savings_ratio = list_size / generator_size\n",
        "print(f\"💾 Экономия памяти: {savings_ratio:.0f}x раз!\")\n",
        "\n",
        "print(f\"\\n📊 Список занимает {list_size:,} байт\")\n",
        "print(f\"📊 Генератор занимает {generator_size:,} байт\")\n",
        "print(f\"🎯 Разница: {list_size - generator_size:,} байт\")\n",
        "\n",
        "# Освобождаем память\n",
        "del large_list\n",
        "print(\"\\n✅ Список удалён из памяти\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Пайплайны обработки данных {#pipelines}\n",
        "\n",
        "Генераторы отлично подходят для создания **пайплайнов обработки данных** - цепочек функций, где каждая стадия обрабатывает данные и передаёт их дальше.\n",
        "\n",
        "С `encode_iterable` можно создавать сложные пайплайны для предобработки текста перед токенизацией.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример пайплайна обработки данных\n",
        "import re\n",
        "\n",
        "def data_source():\n",
        "    \"\"\"Источник данных - различные тексты\"\"\"\n",
        "    texts = [\n",
        "        \"   Привет, мир!   \",\n",
        "        \"Это очень ДЛИННЫЙ текст с множеством слов\",\n",
        "        \"короткий\",\n",
        "        \"ТЕКСТ В ВЕРХНЕМ РЕГИСТРЕ!!!\",\n",
        "        \"   \\t\\n   пустой текст   \\n\\t   \",\n",
        "        \"Текст с цифрами 123 и символами @#$%\",\n",
        "        \"Ещё один нормальный текст для обработки\"\n",
        "    ]\n",
        "    for text in texts:\n",
        "        yield text\n",
        "\n",
        "def clean_whitespace(texts):\n",
        "    \"\"\"Стадия 1: Очистка от лишних пробелов\"\"\"\n",
        "    for text in texts:\n",
        "        cleaned = text.strip()\n",
        "        if cleaned:  # Пропускаем пустые строки\n",
        "            yield cleaned\n",
        "\n",
        "def normalize_case(texts):\n",
        "    \"\"\"Стадия 2: Нормализация регистра\"\"\"\n",
        "    for text in texts:\n",
        "        yield text.lower()\n",
        "\n",
        "def filter_by_length(texts, min_length=5, max_length=50):\n",
        "    \"\"\"Стадия 3: Фильтрация по длине\"\"\"\n",
        "    for text in texts:\n",
        "        if min_length <= len(text) <= max_length:\n",
        "            yield text\n",
        "\n",
        "def remove_special_chars(texts):\n",
        "    \"\"\"Стадия 4: Удаление специальных символов\"\"\"\n",
        "    for text in texts:\n",
        "        # Оставляем только буквы, цифры и пробелы\n",
        "        cleaned = re.sub(r'[^а-яё\\w\\s]', '', text, flags=re.IGNORECASE)\n",
        "        yield cleaned\n",
        "\n",
        "def add_prefix(texts, prefix=\"[ОБРАБОТАНО]\"):\n",
        "    \"\"\"Стадия 5: Добавление префикса\"\"\"\n",
        "    for text in texts:\n",
        "        yield f\"{prefix} {text}\"\n",
        "\n",
        "# Создаём пайплайн\n",
        "print(\"=== Пайплайн обработки текста ===\")\n",
        "print(\"Источник -> Очистка -> Нормализация -> Фильтрация -> Удаление символов -> Префикс -> Токенизация\")\n",
        "print()\n",
        "\n",
        "# Соединяем все стадии\n",
        "source = data_source()\n",
        "stage1 = clean_whitespace(source)\n",
        "stage2 = normalize_case(stage1)\n",
        "stage3 = filter_by_length(stage2, min_length=10, max_length=60)\n",
        "stage4 = remove_special_chars(stage3)\n",
        "stage5 = add_prefix(stage4)\n",
        "\n",
        "# Обрабатываем пайплайн\n",
        "print(\"Результаты пайплайна:\")\n",
        "processed_texts = list(stage5)\n",
        "for i, text in enumerate(processed_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "\n",
        "print(f\"\\nИз 7 исходных текстов получили {len(processed_texts)} обработанных\")\n",
        "\n",
        "# В реальном приложении здесь был бы вызов:\n",
        "# tokenizer = Tokenizer.from_files('vocab.txt', 'merges.txt')\n",
        "# Пересоздаём пайплайн для токенизации\n",
        "final_pipeline = add_prefix(remove_special_chars(filter_by_length(normalize_case(clean_whitespace(data_source())))))\n",
        "# token_stream = tokenizer.encode_iterable(final_pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример с itertools - продвинутая работа с итераторами\n",
        "import itertools\n",
        "\n",
        "def infinite_text_generator():\n",
        "    \"\"\"Бесконечный генератор текста\"\"\"\n",
        "    templates = [\n",
        "        \"Сообщение номер {}\",\n",
        "        \"Документ под номером {}\",\n",
        "        \"Текст для обработки: {}\",\n",
        "        \"Строка данных № {}\"\n",
        "    ]\n",
        "    counter = 1\n",
        "    while True:\n",
        "        template = templates[counter % len(templates)]\n",
        "        yield template.format(counter)\n",
        "        counter += 1\n",
        "\n",
        "print(\"=== Работа с бесконечными последовательностями ===\")\n",
        "\n",
        "# 1. Ограничиваем бесконечную последовательность\n",
        "infinite_texts = infinite_text_generator()\n",
        "limited_texts = itertools.islice(infinite_texts, 5)  # Берём только первые 5\n",
        "\n",
        "print(\"Первые 5 элементов из бесконечной последовательности:\")\n",
        "for text in limited_texts:\n",
        "    print(f\"- {text}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# 2. Группировка данных\n",
        "def categorized_texts():\n",
        "    \"\"\"Генерируем тексты с категориями\"\"\"\n",
        "    data = [\n",
        "        (\"новости\", \"Сегодня хорошая погода\"),\n",
        "        (\"новости\", \"Завтра будет дождь\"),\n",
        "        (\"техника\", \"Новый iPhone выпущен\"),\n",
        "        (\"техника\", \"Python 3.12 доступен\"),\n",
        "        (\"спорт\", \"Футбольный матч завершён\"),\n",
        "        (\"спорт\", \"Теннисный турнир начался\")\n",
        "    ]\n",
        "    for category, text in data:\n",
        "        yield category, text\n",
        "\n",
        "print(\"=== Группировка текстов по категориям ===\")\n",
        "texts_with_categories = categorized_texts()\n",
        "\n",
        "# Группируем по категориям\n",
        "grouped = itertools.groupby(texts_with_categories, key=lambda x: x[0])\n",
        "\n",
        "for category, group in grouped:\n",
        "    print(f\"📂 Категория: {category}\")\n",
        "    texts_in_category = [text for _, text in group]\n",
        "    for text in texts_in_category:\n",
        "        print(f\"  - {text}\")\n",
        "    \n",
        "    # Здесь можно применить encode_iterable к текстам категории:\n",
        "    # token_stream = tokenizer.encode_iterable(texts_in_category)\n",
        "    print()\n",
        "\n",
        "# 3. Цепочка итераторов\n",
        "print(\"=== Объединение нескольких источников данных ===\")\n",
        "\n",
        "def source1():\n",
        "    yield \"Первый источник: документ 1\"\n",
        "    yield \"Первый источник: документ 2\"\n",
        "\n",
        "def source2():\n",
        "    yield \"Второй источник: файл A\"\n",
        "    yield \"Второй источник: файл B\"\n",
        "\n",
        "def source3():\n",
        "    yield \"Третий источник: запись X\"\n",
        "    yield \"Третий источник: запись Y\"\n",
        "\n",
        "# Объединяем все источники в один поток\n",
        "combined_stream = itertools.chain(source1(), source2(), source3())\n",
        "\n",
        "print(\"Объединённый поток данных:\")\n",
        "for text in combined_stream:\n",
        "    print(f\"📄 {text}\")\n",
        "\n",
        "# Этот объединённый поток можно передать в encode_iterable:\n",
        "# token_stream = tokenizer.encode_iterable(combined_stream)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎯 Резюме: Когда использовать encode_iterable\n",
        "\n",
        "### ✅ **Идеальные сценарии для encode_iterable:**\n",
        "\n",
        "1. **🗂️ Обработка больших файлов** - Читать и токенизировать файлы построчно без загрузки в память\n",
        "2. **🌊 Потоковая обработка** - Обработка данных по мере их поступления (чаты, API, сенсоры)\n",
        "3. **💾 Экономия памяти** - Когда у вас много текстовых данных, но ограниченная память\n",
        "4. **🔄 Пайплайны данных** - Создание цепочек обработки текста с предварительными фильтрами\n",
        "5. **♾️ Бесконечные потоки** - Работа с потенциально бесконечными источниками данных\n",
        "6. **🎯 Ленивые вычисления** - Когда не все данные нужны сразу\n",
        "\n",
        "### 🔑 **Ключевые концепции:**\n",
        "\n",
        "- **Iterable** - объект, который можно итерировать (`__iter__()`)\n",
        "- **Iterator** - объект, который производит значения (`__next__()`, `__iter__()`)  \n",
        "- **Generator** - функция с `yield`, автоматически создающая итератор\n",
        "- **Lazy Evaluation** - вычисления выполняются только по требованию\n",
        "\n",
        "### 💡 **Практические преимущества:**\n",
        "\n",
        "```python\n",
        "# Вместо:\n",
        "texts = [read_all_lines_from_file()]  # Загружаем всё в память\n",
        "tokens = tokenizer.encode(texts)\n",
        "\n",
        "# Используйте:\n",
        "text_stream = read_file_lines()       # Генератор строк\n",
        "token_stream = tokenizer.encode_iterable(text_stream)  # Ленивая токенизация\n",
        "```\n",
        "\n",
        "### 🚀 **Результат:**\n",
        "- **Меньше потребление памяти** (в сотни раз!)\n",
        "- **Возможность обработки файлов любого размера**\n",
        "- **Гибкие пайплайны обработки данных**\n",
        "- **Потоковая обработка в реальном времени**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
